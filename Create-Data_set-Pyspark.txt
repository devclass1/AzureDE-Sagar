from pyspark.sql.types import StructType, StructField, StringType, DoubleType

# Define the schema of the data set
schema = StructType([
    StructField("productID", StringType(), True),
    StructField("Price", DoubleType(), True),
    StructField("City", StringType(), True),
    StructField("Discount", DoubleType(), True)
])

# Create the data set
data = [("A1", 100.0, "New York", 0.1),
        ("B2", 200.0, "Los Angeles", 0.2),
        ("C3", 300.0, "Chicago", 0.3),
        ("D4", 400.0, "Houston", 0.4),
        ("E5", 500.0, "Philadelphia", 0.5),
        ("F6", 600.0, "Phoenix", 0.6),
        ("G7", 700.0, "San Antonio", 0.7),
        ("H8", 800.0, "San Diego", 0.8),
        ("I9", 900.0, "Dallas", 0.9),
        ("J10", 1000.0, "San Jose", 0.1),
        ("K11", 1100.0, "Austin", 0.2),
        ("L12", 1200.0, "Jacksonville", 0.3),
        ("M13", 1300.0, "Fort Worth", 0.4),
        ("N14", 1400.0, "Columbus", 0.5),
        ("O15", 1500.0, "San Francisco", 0.6),
        ("P16", 1600.0, "Charlotte", 0.7),
        ("Q17", 1700.0, "Indianapolis", 0.8),
        ("R18", 1800.0, "Seattle", 0.9),
        ("S19", 1900.0, "Denver", 0.1),
        ("T20", 2000.0, "Washington DC", 0.2),
        ("U21", 2100.0, "Boston", 0.3),
        ("V22", 2200.0, "Nashville", 0.4),
        ("W23", 2300.0, "El Paso", 0.5),
        ("X24", 2400.0, "Detroit", 0.6),
        ("Y25", 2500.0, "Memphis", 0.7)]

df = spark.createDataFrame(data, schema=schema)

# Show the data set
df.show()
